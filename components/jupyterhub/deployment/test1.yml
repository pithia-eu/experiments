tosca_definitions_version: tosca_simple_yaml_1_2
imports:
- https://raw.githubusercontent.com/micado-scale/tosca/develop/micado_types.yaml
repositories:
  docker_hub: https://hub.docker.com/
description: 'Generated from K8s manifests: deployment.yaml'
topology_template:
  node_templates:
    kube-state-metrics-clusterrole:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: rbac.authorization.k8s.io/v1
              kind: ClusterRole
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
              rules:
              - apiGroups:
                - ''
                resources:
                - configmaps
                - secrets
                - nodes
                - pods
                - services
                - resourcequotas
                - replicationcontrollers
                - limitranges
                - persistentvolumeclaims
                - persistentvolumes
                - namespaces
                - endpoints
                verbs:
                - list
                - watch
              - apiGroups:
                - extensions
                resources:
                - daemonsets
                - deployments
                - replicasets
                - ingresses
                verbs:
                - list
                - watch
              - apiGroups:
                - apps
                resources:
                - statefulsets
                - daemonsets
                - deployments
                - replicasets
                verbs:
                - list
                - watch
              - apiGroups:
                - batch
                resources:
                - cronjobs
                - jobs
                verbs:
                - list
                - watch
              - apiGroups:
                - autoscaling
                resources:
                - horizontalpodautoscalers
                verbs:
                - list
                - watch
              - apiGroups:
                - authentication.k8s.io
                resources:
                - tokenreviews
                verbs:
                - create
              - apiGroups:
                - authorization.k8s.io
                resources:
                - subjectaccessreviews
                verbs:
                - create
              - apiGroups:
                - policy
                resources:
                - poddisruptionbudgets
                verbs:
                - list
                - watch
              - apiGroups:
                - certificates.k8s.io
                resources:
                - certificatesigningrequests
                verbs:
                - list
                - watch
              - apiGroups:
                - storage.k8s.io
                resources:
                - storageclasses
                - volumeattachments
                verbs:
                - list
                - watch
              - apiGroups:
                - admissionregistration.k8s.io
                resources:
                - mutatingwebhookconfigurations
                - validatingwebhookconfigurations
                verbs:
                - list
                - watch
              - apiGroups:
                - networking.k8s.io
                resources:
                - networkpolicies
                verbs:
                - list
                - watch
    kube-state-metrics-clusterrolebinding:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: rbac.authorization.k8s.io/v1
              kind: ClusterRoleBinding
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
              roleRef:
                apiGroup: rbac.authorization.k8s.io
                kind: ClusterRole
                name: kube-state-metrics
              subjects:
              - kind: ServiceAccount
                name: kube-state-metrics
                namespace: kube-system
    kube-state-metrics-serviceaccount:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: ServiceAccount
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
                namespace: kube-system
    kube-state-metrics-deployment:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: apps/v1
              kind: Deployment
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
                namespace: kube-system
              spec:
                replicas: 1
                selector:
                  matchLabels:
                    app.kubernetes.io/name: kube-state-metrics
                template:
                  metadata:
                    labels:
                      app.kubernetes.io/name: kube-state-metrics
                      app.kubernetes.io/version: v1.8.0
                  spec:
                    tolerations:
                    - key: node-role.kubernetes.io/master
                      effect: NoSchedule
                    containers:
                    - image: quay.io/coreos/kube-state-metrics:v1.8.0
                      livenessProbe:
                        httpGet:
                          path: /healthz
                          port: 8080
                        initialDelaySeconds: 5
                        timeoutSeconds: 5
                      name: kube-state-metrics
                      ports:
                      - containerPort: 8080
                        name: http-metrics
                      - containerPort: 8081
                        name: telemetry
                      readinessProbe:
                        httpGet:
                          path: /
                          port: 8081
                        initialDelaySeconds: 5
                        timeoutSeconds: 5
                    nodeSelector:
                      kubernetes.io/os: linux
                    serviceAccountName: kube-state-metrics
    kube-state-metrics-service:
      type: tosca.nodes.MiCADO.Kubernetes
      interfaces:
        Kubernetes:
          create:
            inputs:
              apiVersion: v1
              kind: Service
              metadata:
                labels:
                  app.kubernetes.io/name: kube-state-metrics
                  app.kubernetes.io/version: v1.8.0
                name: kube-state-metrics
                namespace: kube-system
              spec:
                clusterIP: None
                ports:
                - name: http-metrics
                  port: 8080
                  targetPort: http-metrics
                - name: telemetry
                  port: 8081
                  targetPort: telemetry
                selector:
                  app.kubernetes.io/name: kube-state-metrics

    #Don't Change these fields

    jupyterhub-configmap:
      type: tosca.nodes.MiCADO.Container.Config.Kubernetes
      properties:
        data:
          jupyterhub.py: "import os

                          ## Generic

                          c.JupyterHub.admin_access = True

                          c.Spawner.default_url = '/lab'

                          # Network Configuration

                          c.JupyterHub.spawner_class = os.environ['SPAWNER_CLASS']

                          c.JupyterHub.hub_ip = os.environ['HUB_IP']

                          c.KubeSpawner.hub_connect_ip = os.environ['K8S_HUB_API_IP']

                          # Start Notebook in user directory

                          c.KubeSpawner.notebook_dir = '/home/jovyan/{username}'

                          c.KubeSpawner.working_dir = '/home/jovyan/{username}'

                          # PoD Requirements for the Jupyter notbooks

                          c.KubeSpawner.cpu_guarantee = os.environ.get('CPU_MIN')

                          c.KubeSpawner.cpu_limit = os.environ.get('CPU_MAX')

                          c.KubeSpawner.mem_guarantee = os.environ.get('MEM_MIN')

                          c.KubeSpawner.mem_limit = os.environ.get('MEM_MAX')

                          c.KubeSpawner.storage_capacity =  os.environ.get('STORAGE_CAPACITY')

                          c.KubeSpawner.start_timeout = 60*10

                          # Config the NFS storage volume for Jupyter notebooks

                          nfs_server = os.environ.get('NFS_SERVER')

                          nfs_path = os.environ.get('NFS_PATH')

                          c.KubeSpawner.fs_gid = os.environ.get('FS_GID')

                          c.KubeSpawner.gid = os.environ.get('GID')

                          c.KubeSpawner.volumes = [{'name': 'home','nfs': {'server': nfs_server,'path': nfs_path}}]

                          c.KubeSpawner.volume_mounts = [{'name': 'home','mountPath': '/home/jovyan'}]

                          # Only spawn the jupyter notebook on nodes are labels as 'jupyter-node'

                          c.KubeSpawner.node_selector = {'micado.eu/node_type':'jupyter-node'}

                          "

    jupyterhub:
      type: tosca.nodes.MiCADO.Container.Application.Docker.Deployment
      properties:
        name: jupyterhub
        image: "jupyterhub/jupyterhub:1.2"
        lifecycle:
          postStart:
            exec:
              command: [ "/bin/sh", "-c", "apt update && apt install sudo && sudo cp /etc/config/jupyterhub.py jupyterhub.py && pip install jupyterhub-kubespawner" ]
        env:
          - name: HUB_IP
            value: 0.0.0.0
          - name: SPAWNER_CLASS
            value: "kubespawner.KubeSpawner"
          - name: K8S_HUB_API_IP
            value: "jupyterhub-clusterip"
          - name: NFS_SERVER
            value: 18.133.255.231
          - name: NFS_PATH
            value: "/mnt/nfs_share/users"
        ports:
          - port: 8000
            #This is the public port to access the jupyterhub
            nodePort: 30001
          - port: 8081

      requirements:
       - host: jupyterhub-node
       - volume:
            node: nfs-volume
            relationship:
              type: tosca.relationships.AttachesTo
              properties:
                location: /data
       - volume:
           node: jupyterhub-configmap
           relationship:
             type: tosca.relationships.AttachesTo
             properties:
               location: /etc/config

    jupyterhub-node:
      type: tosca.nodes.MiCADO.EC2.Compute
      properties:
        region_name: eu-west-2 #ADD_YOUR_REGION_NAME
        image_id: ami-09a1e275e350acf38 #ADD_YOUR_IMAGE_ID
        instance_type: t2.medium #ADD_YOUR_IMAGE_ID
        key_name: Dimitris-key #ADD_YOUR_KEY_NAME
        #Allow all traffic to Jupyterhub
        security_group_ids:
          - sg-075395b33ac05ee99 #ADD_YOUR_SECURITY_GROUP_ID
        #Don't change these fields
        context:
          append: true
          cloud_config: |
            runcmd:
            - sudo apt-get install -y nfs-common

      interfaces:
        Occopus:
          create:
            inputs:
              endpoint: https://ec2.eu-west-2.amazonaws.com  #ADD_YOUR_ENDPOINT

    #Don't change these fields
    nfs-volume:
      type: tosca.nodes.MiCADO.Container.Volume.NFS
      properties:
        server: 18.133.255.231
        path: /mnt/nfs_share

    notebook-node:
      type: tosca.nodes.MiCADO.EC2.Compute
      properties:
        region_name: eu-west-2
        image_id: ami-09a1e275e350acf38
        instance_type: t2.medium
        key_name: Dimitris-key
        security_group_ids:
          - sg-075395b33ac05ee99
        #Don't change these fields
        context:
          append: true
          cloud_config: |
            runcmd:
            - sudo apt-get install -y nfs-common

      interfaces:
        Occopus:
          create:
            inputs:
              endpoint: https://ec2.eu-west-2.amazonaws.com

  policies:
    - monitoring:
        type: tosca.policies.Monitoring.MiCADO
        properties:
          enable_container_metrics: true
          enable_node_metrics: true
    - scalability:
        type: tosca.policies.Scaling.MiCADO.VirtualMachine.jupyterhub
        targets: [ jupyterhub-node ]
        properties:
          sources:
          - 'kube-state-metrics.kube-system.svc.cluster.local:8080'
          constants:
            NODE_NAME: 'jupyterhub-node'
            SCALE_UP_IF_FREE_SLOTS_ARE_LESS_THAN: '2'
            SCALE_DOWN_IF_FREE_SLOTS_ARE_LESS_THAN: '4'
            PODS_PER_NODE_AVG: '6'
          queries:
            #active_jhub_users: 'count(sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node) + sum(kube_pod_info{node=~"jupyterhub-node.*",pod=~"jupyter.*"}) by (node)) OR on() vector(0)'
            schedulable_nodes:  'count(sum(kube_node_status_condition{condition="Ready",status="true",node=~"jupyterhub-node.*"} == 1) by (node) and sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node)) OR on() vector(0)'
            #users_on_schedulable_nodes
            #free_slots_on_schedulable_nodes
            unschedulable_nodes: 'count(sum(kube_node_status_condition{condition="Ready",status="true",node=~"jupyterhub-node.*"} == 1) by (node) and sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 1) by (node)) OR on() vector(0)'
            #users_on_schedulable_nodes
            #free_slots_on_unschedulable_nodes
          min_instances: 1
          max_instances: 2

policy_types:
  tosca.policies.Scaling.MiCADO.VirtualMachine.jupyterhub:
    derived_from: tosca.policies.Scaling.MiCADO
    description: base MiCADO policy defining data sources, constants, queries, alerts, limits and rules
    properties:
      alerts:
        type: list
        description: pre-define alerts for JHUB
        default:
        - alert: jhub_overloaded
          expr: '((count(sum(kube_node_status_condition{condition="Ready",status="true",node=~"jupyterhub-node.*"} == 1) by (node) and sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node)) * {{PODS_PER_NODE_AVG}} OR on() vector(0)) - (sum(sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node) + sum(kube_pod_info{node=~"jupyterhub-node.*",pod=~"jupyter-.*"}) by (node)) OR on() vector(0))) < {{SCALE_UP_IF_FREE_SLOTS_ARE_LESS_THAN}}'
          for: 1m
        - alert: jhub_underloaded
          expr: '((count(sum(kube_node_status_condition{condition="Ready",status="true",node=~"jupyterhub-node.*"} == 1) by (node) and sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node)) * {{PODS_PER_NODE_AVG}} OR on() vector(0)) - (sum(sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 0) by (node) + sum(kube_pod_info{node=~"jupyterhub-node.*",pod=~"jupyter-.*"}) by (node)) OR on() vector(0))) > {{SCALE_DOWN_IF_FREE_SLOTS_ARE_LESS_THAN}}'
          for: 3m
        - alert: jhub_node_unschedulable
          expr: '(count(sum(kube_node_status_condition{condition="Ready",status="true",node=~"jupyterhub-node.*"} == 1) by (node) and sum(max_over_time(kube_node_spec_unschedulable{node=~"jupyterhub-node.*"}[30s]) == 1) by (node)) OR on() vector(0)) > 0'
        required: true
      scaling_rule:
        type: string
        description: pre-define scaling rule for JHUB
        default: |

          # JHUB status
          print("JupyterHub policy")
          print("Current status:")
          print("from MiCADO, active JHUB nodes: {}".format(len(m_nodes)))
          print("from MiCADO, required JHUB nodes: {}".format(m_node_count))
          print("from MiCADO, time since required JHUB node count changed: {}".format(m_time_since_node_count_changed))
          #print("from Prometheus, JHUB users: {}".format(active_jhub_users))
          print("from Prometheus, schedulable  nodes: {}".format(schedulable_nodes))
          #print("from Prometheus, users on schedulable  nodes: {}".format(schedulable_nodes))
          #print("from Prometheus, free slots on schedulable  nodes: {}".format(schedulable_nodes))
          print("from Prometheus, unschedulable  nodes: {}".format(unschedulable_nodes))
          #print("from Prometheus, users on unschedulable  nodes: {}".format(unschedulable_nodes))
          #print("from Prometheus, free slots on unschedulable  nodes: {}".format(unschedulable_nodes))

          print(max_instances)

          # Checking if MiCADO performs changes in the cluster nodes or if there are unschedulable nodes
          print("Checking if policy can be applied...")
          if (len(m_nodes) <= m_node_count and m_time_since_node_count_changed > 60) or jhub_node_unschedulable:
            print("Applying...")

            # Query JHUB nodes and pods from kubernetes
            kube = pykube.HTTPClient(pykube.KubeConfig.from_file("/root/.kube/config"))
            nodes = pykube.Node.objects(kube).filter(selector={"micado.eu/node_type":"jupyterhub-node"})
            pods = pykube.Pod.objects(kube).filter(selector={"app":"jupyterhub"})

            # Create two dictionaries one for schedulable and one for unschedulable nodes
            # for key use the name of the nodes and for values the sum of the running pods on each node
            print("Collecting JHUB nodes and pods information...")
            pods_per_schedulable_nodes = dict()
            pods_per_unschedulable_nodes = dict()
            for node in nodes:
              #node.obj['status']['conditions'][4].get('status') and
              if node.obj['status']['conditions'][4].get('status') == "True":
                if node.obj["spec"].get("unschedulable"):
                  pods_per_unschedulable_nodes[node.obj["metadata"].get("name")] = 0
                else:
                  pods_per_schedulable_nodes[node.obj["metadata"].get("name")] = 0
            for pod in pods:
              if pod.obj["spec"].get("nodeName") in pods_per_unschedulable_nodes:
                pods_per_unschedulable_nodes[pod.obj["spec"].get("nodeName")] += 1
              elif pod.obj["spec"].get("nodeName") in pods_per_schedulable_nodes:
                pods_per_schedulable_nodes[pod.obj["spec"].get("nodeName")] += 1

            print("Schedulable nodes:")
            print(pods_per_schedulable_nodes)
            if pods_per_schedulable_nodes:
              print("least utilised schedulable node: {}".format(min(pods_per_schedulable_nodes, key=pods_per_schedulable_nodes.get)))
              print("pods: {}".format(min(pods_per_schedulable_nodes.values())))
            print("Unschedulable nodes:")
            print(pods_per_unschedulable_nodes)
            if pods_per_unschedulable_nodes:
              print("most utilised unschedulable node: {}".format(max(pods_per_unschedulable_nodes, key=pods_per_unschedulable_nodes.get)))
              print("pods: {}".format(max(pods_per_unschedulable_nodes.values())))

            # Checking if Prometheus fires an overloaded alert
            if jhub_overloaded: # and m_node != max_instance
              print("JupyterHub is overloaded")
              # If there are unschedulable nodes make SCHEDULABLE the most utilised node, else ADD a new node
              if pods_per_unschedulable_nodes:
                most_utilised_unschedulable_node = max(pods_per_unschedulable_nodes, key=pods_per_unschedulable_nodes.get)
                print("TEST most_utilised_unschedulable_node", most_utilised_unschedulable_node)
                for node in nodes:
                  if node.obj["metadata"].get("name") == most_utilised_unschedulable_node:
                    print("Making schedulable the most utilised unschedulable node: {}".format(node))
                    node.uncordon()
                    pods_per_unschedulable_nodes.pop(most_utilised_unschedulable_node, node)
                    print("Test after pop!!", pods_per_unschedulable_nodes)
                    break
              else:
                print("Adding a new node (if not max instances)..")
                m_node_count+=1

            # Checking if Prometheus fires an underloaded alert
            elif jhub_underloaded and len(m_nodes)>1:
              print("JupyterHub is underloaded")
              # Make the least utilised node UNSCHEDULABLE
              for node in nodes:
                if node.obj["metadata"].get("name") == min(pods_per_schedulable_nodes, key=pods_per_schedulable_nodes.get):
                  print("Making unschedulable the leas utilised node: {}".format(node))
                  node.cordon()
                  break
            else:
              print("No active alerts on Prometheus")

            # If there is an empty and unschedulable nodes REMOVE it
            if pods_per_unschedulable_nodes and min(pods_per_unschedulable_nodes.values()) == 0:
              for node in nodes:
                if node.obj["metadata"].get("name") == min(pods_per_unschedulable_nodes, key=pods_per_unschedulable_nodes.get):
                  m_nodes_todrop.append(node.obj["status"]["addresses"][0].get("address"))
                  print("Removing a empty node: {}".format(node))
                  break
            else:
              print("No empty nodes")

          else:
            print('Transient phase, skipping...')

        required: true
